{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"MA2221 \u2013 Foundational Mathematics for Machine Learning","text":""},{"location":"#i-course-information","title":"\u2139\ufe0f Course Information","text":"<p>Instructor: Biswarup Biswas Institution: Mahindra University</p>"},{"location":"#class-schedule","title":"\ud83d\udcc5 Class Schedule","text":"Session Type Day Time Venue Practical Monday 15:35 \u2013 17:30 Computer Lab 3 Lecture Tuesday 09:25 \u2013 10:20 ELT6 Lecture Wednesday 10:35 \u2013 11:30 ELT6 Lecture Thursday 14:35 \u2013 15:30 ELT6"},{"location":"#about-the-course","title":"\ud83c\udfaf About the Course","text":"<p>This course develops a mathematical perspective on machine learning, focusing on how data and models are described using linear algebra, geometry, probability, and optimization. It emphasizes understanding learning as a problem of fitting and approximating functions in high dimensional spaces, while also giving students hands on experience in applying these ideas to real data. The course equips students with the mathematical language and practical insight needed to analyze and use modern machine learning methods in a clear and principled way.</p>"},{"location":"lectures/","title":"\ud83d\udcda Lecture Schedule","text":"No Date Topic Materials 01 20-01-2026 Foundations: Mapping Mathematics to Machine Learning \u23f3 Pending"},{"location":"lectures/#note","title":"\ud83d\udccc Note","text":"<p>Slides will be uploaded after each session.</p>"},{"location":"syllabus/","title":"Mathematics for Machine Learning","text":""},{"location":"syllabus/#part-i-mathematical-foundations","title":"PART I \u2014 Mathematical Foundations","text":""},{"location":"syllabus/#module-1-introduction","title":"Module 1 \u2014 Introduction","text":"<p>Class 1 - Machine learning as function approximation and data fitting - Training, prediction, and model error - Why geometry, probability, and optimization matter  </p>"},{"location":"syllabus/#module-2-linear-algebra-for-data","title":"Module 2 \u2014 Linear Algebra for Data","text":"<p>Class 2 \u2013 Vectors and data representation in high dimensions Class 3 \u2013 Linear maps as feature transformations Class 4 \u2013 Rank, conditioning, and stability of models Class 5 \u2013 Subspaces as hypothesis spaces Class 6 \u2013 Basis change and coordinate systems Class 7 \u2013 Null spaces and information loss Class 8 \u2013 Geometry of linear transformations  </p>"},{"location":"syllabus/#module-3-analytic-geometry-of-data","title":"Module 3 \u2014 Analytic Geometry of Data","text":"<p>Class 9 \u2013 Norms, distances, and similarity measures Class 10 \u2013 Inner products and cosine similarity Class 11 \u2013 Orthogonality and decorrelation Class 12 \u2013 Projections and least-squares fitting Class 13 \u2013 Geometry of data clouds  </p>"},{"location":"syllabus/#module-4-matrix-decompositions","title":"Module 4 \u2014 Matrix Decompositions","text":"<p>Class 14 \u2013 Linear operators and invariant directions Class 15 \u2013 Eigenvalues and eigenvectors Class 16 \u2013 Symmetric matrices and quadratic forms Class 17 \u2013 Singular Value Decomposition (SVD) Class 18 \u2013 SVD for optimal low-rank data representation  </p>"},{"location":"syllabus/#module-5-vector-calculus-for-learning","title":"Module 5 \u2014 Vector Calculus for Learning","text":"<p>Class 19 \u2013 Multivariate functions and loss surfaces Class 20 \u2013 Gradients as directions of steepest descent Class 21 \u2013 Hessians, curvature, and conditioning Class 22 \u2013 Gradient-based optimization  </p>"},{"location":"syllabus/#module-6-probability-for-data","title":"Module 6 \u2014 Probability for Data","text":"<p>Class 23 \u2013 Random variables as data generators Class 24 \u2013 Mean, variance, and covariance Class 25 \u2013 Multivariate Gaussian distributions Class 26 \u2013 Probability as a model of uncertainty  </p>"},{"location":"syllabus/#part-ii-core-machine-learning","title":"PART II \u2014 Core Machine Learning","text":""},{"location":"syllabus/#module-7-when-models-meet-data","title":"Module 7 \u2014 When Models Meet Data","text":"<p>Class 27 \u2013 Training, testing, and generalization Class 28 \u2013 Loss functions and risk minimization Class 29 \u2013 Overfitting, bias\u2013variance, regularization  </p>"},{"location":"syllabus/#module-8-linear-regression","title":"Module 8 \u2014 Linear Regression","text":"<p>Class 30 \u2013 Linear regression as projection Class 31 \u2013 Least-squares and normal equations Class 32 \u2013 Ridge regression and stability  </p>"},{"location":"syllabus/#module-9-principal-component-analysis","title":"Module 9 \u2014 Principal Component Analysis","text":"<p>Class 33 \u2013 Variance, covariance, and principal directions Class 34 \u2013 Eigenvectors as principal components Class 35 \u2013 PCA via SVD Class 36 \u2013 Data compression and visualization  </p>"},{"location":"syllabus/#module-10-gaussian-mixture-models","title":"Module 10 \u2014 Gaussian Mixture Models","text":"<p>Class 37 \u2013 Probabilistic clustering and mixture models Class 38 \u2013 Gaussian mixtures Class 39 \u2013 Expectation\u2013Maximization (EM) algorithm  </p>"},{"location":"syllabus/#module-11-support-vector-machines","title":"Module 11 \u2014 Support Vector Machines","text":"<p>Class 40 \u2013 Maximum margin classification and separating hyperplanes Class 41 \u2013 Dual formulation and support vectors Class 42 \u2013 Kernel methods and nonlinear decision boundaries  </p>"},{"location":"syllabus/#textbook","title":"Textbook","text":"<p>Deisenroth, M. P., Faisal, A. A., &amp; Ong, C. S. (2020). Mathematics for Machine Learning. Cambridge University Press.</p>"},{"location":"tutorials/","title":"\u270d\ufe0f Tutorial Sheets","text":"No Release Date Topic Download \u2014 \u2014 Coming Soon \u2014"},{"location":"tutorials/#instructions","title":"\ud83d\udee0\ufe0f Instructions","text":"<ul> <li>Tutorial sheets should be solved prior to the weekly lab.</li> </ul>"},{"location":"notebooks/notebook1/","title":"Introduction","text":"<p>You should see something like:</p> <pre>Python 3.x.x\n</pre> In\u00a0[1]: Copied! <pre>2 + 3*4\n</pre>  2 + 3*4 Out[1]: <pre>14</pre>"},{"location":"notebooks/notebook1/#introduction","title":"Introduction\u00b6","text":"<p>In this course, we study how data, geometry, probability, and optimization come together to build learning algorithms.</p> <p>Python will be our computational laboratory for this journey.</p> <p>We will use Python to:</p> <ul> <li>work with vectors and matrices (Modules 2\u20134)</li> <li>visualize geometry of data (Modules 3, 9)</li> <li>compute gradients and optimization steps (Module 5, 8)</li> <li>simulate probability models (Modules 6, 10)</li> </ul>"},{"location":"notebooks/notebook1/#why-python","title":"Why Python?\u00b6","text":"<p>Python is the standard language for scientific computing and machine learning.</p> <p>The core libraries we will use are:</p> Library Purpose NumPy Vectors, matrices, linear algebra Matplotlib Plots and visualizations SciPy Scientific and numerical routines scikit-learn Machine learning algorithms <p>These libraries let us turn mathematical formulas into executable experiments.</p>"},{"location":"notebooks/notebook1/#installing-python-on-your-computer","title":"Installing Python on Your Computer\u00b6","text":"<p>The easiest way to get everything needed for this course is Anaconda.</p>"},{"location":"notebooks/notebook1/#step-1","title":"Step 1\u00b6","text":"<p>Download Anaconda from: https://www.anaconda.com/products/distribution</p> <p>Choose:</p> <ul> <li>Python 3</li> <li>Your operating system (Windows, macOS, or Linux)</li> </ul>"},{"location":"notebooks/notebook1/#step-2","title":"Step 2\u00b6","text":"<p>Install using the default options.</p> <p>This installs:</p> <ul> <li>Python</li> <li>Jupyter Notebook</li> <li>All scientific libraries used in this course</li> </ul>"},{"location":"notebooks/notebook1/#checking-your-installation","title":"Checking Your Installation\u00b6","text":"<p>Open:</p> <ul> <li>Anaconda Prompt (Windows), or</li> <li>Terminal (macOS / Linux)</li> </ul> <p>Type:</p> <pre>python --version\n</pre>"},{"location":"notebooks/notebook1/#starting-jupyter","title":"Starting Jupyter\u00b6","text":"<p>To start Jupyter Notebook, type:</p> <pre>jupyter notebook\n</pre> <p>A new browser tab will open with the notebook dashboard.</p>"},{"location":"notebooks/notebook1/#what-is-a-jupyter-notebook","title":"What is a Jupyter Notebook?\u00b6","text":"<p>A Jupyter notebook is an interactive mathematical document.</p> <p>It can contain:</p> <ul> <li>text and explanations</li> <li>mathematical formulas</li> <li>code</li> <li>plots and figures</li> <li>computed results</li> </ul> <p>In this course, notebooks will serve as a computational extension of the lecture notes. They allow us to connect abstract mathematics with concrete computation.</p>"},{"location":"notebooks/notebook1/#using-google-colab","title":"Using Google Colab\u00b6","text":"<p>If you do not want to install anything on your computer, you can use Google Colab.</p> <p>Google Colab is a free online Jupyter notebook service.</p> <p>To use it:</p> <ol> <li>Go to https://colab.research.google.com</li> <li>Click New Notebook</li> <li>Start writing and running Python code in your browser</li> </ol> <p>All computation runs on Google's servers.</p>"},{"location":"notebooks/notebook1/#local-jupyter-vs-google-colab","title":"Local Jupyter vs Google Colab\u00b6","text":"Option When to use Anaconda + Jupyter When you want everything on your own computer Google Colab When you want instant access from anywhere without installation <p>Both options work equally well for this course.</p>"},{"location":"notebooks/notebook1/#writing-mathematics-in-jupyter","title":"Writing Mathematics in Jupyter\u00b6","text":"<p>Jupyter notebooks allow us to write mathematics using LaTeX.</p> <p>This means we can write formulas in the same way as in textbooks, but inside an interactive document.</p>"},{"location":"notebooks/notebook1/#cells-in-a-jupyter-notebook","title":"Cells in a Jupyter Notebook\u00b6","text":"<p>A Jupyter notebook is made of cells.</p> <p>There are two types of cells we will use:</p> <ul> <li>Markdown cells \u2014 for text, explanations, and mathematics written in LaTeX</li> <li>Code cells \u2014 for Python computations</li> </ul> <p>When we write formulas using LaTeX, we always use Markdown cells.</p>"},{"location":"notebooks/notebook1/#inline-mathematics","title":"Inline Mathematics\u00b6","text":"<p>To write mathematics inside a sentence, we use dollar ($) signs.</p> <p>For example, we can write the square of a number as $x^2$, or a vector as $\\mathbf{x}$, or an inner product as $\\langle x, y \\rangle$.</p> <p>These formulas appear directly inside the text. We wrote this as</p> <pre>For example, we can write the square of a number as $x^2$,\nor a vector as $\\mathbf{x}$, or an inner product as $\\langle x, y \\rangle$.\n</pre>"},{"location":"notebooks/notebook1/#displayed-equations","title":"Displayed Equations\u00b6","text":"<p>When a formula is important, we write it on its own line.</p> <p>For example, a quadratic function can be written as</p> <p>$$ f(x) = x^2 + 3x + 1. $$</p> <p>Loss functions, regression models, and probability densities will usually be written in this form. The above equation is produced by typing the following in a Markdown cell:</p> <pre>$$\nf(x) = x^2 + 3x + 1\n$$\n</pre>"},{"location":"notebooks/notebook1/#fractions","title":"Fractions\u00b6","text":"<p>Fractions are written using the \\frac command.</p> <p>For example,</p> <p>$$ \\frac{a}{b}, \\quad \\frac{x+1}{x-1} $$</p> <p>These are written as</p> <pre>\\frac{a}{b}\n\\frac{x+1}{x-1}\n</pre>"},{"location":"notebooks/notebook1/#superscripts-and-subscripts","title":"Superscripts and Subscripts\u00b6","text":"<p>Powers and indices are written using ^ and _.</p> <p>For example,</p> <p>$$ x^2,\\quad x^3,\\quad x_i,\\quad a_{ij} $$</p> <p>These are written as</p> <pre>x^2\nx^3\nx_i\na_{ij}\n</pre>"},{"location":"notebooks/notebook1/#greek-letters","title":"Greek Letters\u00b6","text":"<p>Greek letters are widely used in mathematics.</p> <p>For example,</p> <p>$$ \\alpha,\\; \\beta,\\; \\gamma,\\; \\mu,\\; \\sigma $$</p> <p>These are written as</p> <pre>\\alpha\n\\beta\n\\gamma\n\\mu\n\\sigma\n</pre>"},{"location":"notebooks/notebook1/#common-mathematical-symbols","title":"Common Mathematical Symbols\u00b6","text":"<p>Here are some important mathematical symbols and how to write them in LaTeX.</p>"},{"location":"notebooks/notebook1/#inline-mathematical-symbols","title":"Inline mathematical symbols\u00b6","text":"Mathematical Expression LaTeX Code $\\sum_{i=1}^{n} x_i$ <code>\\sum_{i=1}^{n} x_i</code> $\\mathbf{x}$ <code>\\mathbf{x}</code> $\\langle \\mathbf{x}, \\mathbf{y} \\rangle$ <code>\\langle \\mathbf{x}, \\mathbf{y} \\rangle</code> $\\|\\mathbf{x}\\|$ <code>\\|\\mathbf{x}\\|</code> $\\frac{df}{dx}$ <code>\\frac{df}{dx}</code> $\\nabla f(\\mathbf{x})$ <code>\\nabla f(\\mathbf{x})</code> $\\int_0^1 x^2\\,dx$ <code>\\int_0^1 x^2\\,dx</code> $\\displaystyle \\lim_{x \\to 0} \\frac{\\sin x}{x} = 1$ <code>\\lim_{x \\to 0} \\frac{\\sin x}{x} = 1</code>"},{"location":"notebooks/notebook1/#matrix-notation","title":"Matrix notation\u00b6","text":"<p>$$ A = \\begin{pmatrix} a_{11} &amp; a_{12} \\\\ a_{21} &amp; a_{22} \\end{pmatrix} $$</p> <pre>$$\nA=\\begin{pmatrix}\na_{11} &amp; a_{12} \\\\\na_{21} &amp; a_{22}\n\\end{pmatrix}\n$$\n</pre>"},{"location":"notebooks/notebook1/#the-align-environment-in-latex","title":"The <code>align</code> environment in LaTeX\u00b6","text":"<p>We use <code>align</code> family of environments in LaTeX for writing multi-line display equations with proper alignment.</p>"},{"location":"notebooks/notebook1/#basic-usage","title":"Basic usage\u00b6","text":"<p>Use <code>align</code> when equations should be aligned at a chosen symbol (typically <code>=</code>).</p> <pre>\\begin{align}\n  a &amp;= b + c \\\\\n    &amp;= d + e\n\\end{align}\n</pre> <p>Output: \\begin{align}   a &amp;= b + c \\\\     &amp;= d + e \\end{align}</p> <p>Each line is aligned at <code>&amp;</code> and receives its own equation number.</p>"},{"location":"notebooks/notebook1/#unnumbered-equations","title":"Unnumbered equations\u00b6","text":"<p>Use <code>align*</code> to suppress equation numbering.</p> <pre>\\begin{align*}\n  y' &amp;= f(x,y) \\\\\n     &amp;= x^2 + y\n\\end{align*}\n</pre> <p>Output: \\begin{align*}   y' &amp;= f(x,y) \\\\      &amp;= x^2 + y \\end{align*}</p>"},{"location":"notebooks/notebook1/#controlling-equation-numbers","title":"Controlling equation numbers\u00b6","text":"<p>To number only selected lines, use <code>\\notag</code>.</p> <pre>\\begin{align}\n  y' &amp;= f(x,y) \\notag \\\\\n     &amp;= x^2 + y\n\\end{align}\n</pre> <p>Output: \\begin{align}   y' &amp;= f(x,y) \\notag \\\\      &amp;= x^2 + y \\end{align}</p>"},{"location":"notebooks/notebook1/#long-expressions","title":"Long expressions\u00b6","text":"<p>Long right-hand sides can be split across lines while maintaining alignment.</p> <pre>\\begin{align*}\n  y_{n+1} ={}&amp; y_n + h f(x_n,y_n) \\\\\n             &amp;+ \\frac{h^2}{2} f'(x_n,y_n)\n\\end{align*}\n</pre> <p>Output: \\begin{align*}   y_{n+1} ={}&amp; y_n + h f(x_n,y_n) \\\\              &amp;+ \\frac{h^2}{2} f'(x_n,y_n) \\end{align*}</p>"},{"location":"notebooks/notebook1/#piecewise-functions","title":"Piecewise Functions\u00b6","text":"<p>Piecewise-defined functions are written using the <code>cases</code> environment.</p> <p>For example,</p> <p>$$ f(x) = \\begin{cases} x^2, &amp; x \\ge 0, \\\\ -x,  &amp; x &lt; 0. \\end{cases} $$</p> <p>This is written as</p> <pre>f(x) =\n\\begin{cases}\nx^2, &amp; x \\ge 0, \\\\\n-x,  &amp; x &lt; 0.\n\\end{cases}\n</pre>"},{"location":"notebooks/notebook1/#blackboard-bold-mathbb","title":"Blackboard Bold ( \\mathbb )\u00b6","text":"<p>Some mathematical symbols are written in a special \u201cblackboard bold\u201d style.</p> <p>This is commonly used for standard number systems.</p> <p>For example,</p> <p>$$ \\mathbb{R}, \\quad \\mathbb{N}, \\quad \\mathbb{Z}, \\quad \\mathbb{Q} $$</p> <p>These represent:</p> <ul> <li>$\\mathbb{R}$ \u2014 real numbers</li> <li>$\\mathbb{N}$ \u2014 natural numbers</li> <li>$\\mathbb{Z}$ \u2014 integers</li> <li>$\\mathbb{Q}$ \u2014 rational numbers</li> </ul> <p>They are written as</p> <pre>\\mathbb{R}\n\\mathbb{N}\n\\mathbb{Z}\n\\mathbb{Q}\n</pre>"},{"location":"notebooks/notebook1/#learning-more-latex","title":"Learning More LaTeX\u00b6","text":"<p>By now, you have seen the basic pattern:</p> <ul> <li>You write LaTeX commands inside <code>$...$</code> or <code>$$...$$</code></li> <li>You can also use environments like <code>align</code> and <code>cases</code></li> <li>Jupyter renders them as mathematical symbols and formulas</li> </ul> <p>You are not expected to memorize all LaTeX commands.</p> <p>Whenever you need a new symbol or expression, you can:</p> <ul> <li>search online</li> <li>look it up in documentation</li> <li>or ask a tool like ChatGPT</li> </ul> <p>For example, you can ask: \u201cHow do I write a norm in LaTeX?\u201d \u201cHow do I write a matrix?\u201d \u201cHow do I write a summation?\u201d</p> <p>The important thing is to understand the pattern: type LaTeX \u2192 see mathematics.</p>"},{"location":"notebooks/notebook1/#one-more-thing","title":"One More Thing \ud83d\ude42\u00b6","text":"<p>There\u2019s a small but very handy symbol called \u201cplus\u2013minus\u201d.</p> <p>It looks like</p> <p>$$ \\pm $$</p> <p>and it is typed as</p> <pre>\\pm\n</pre>"},{"location":"notebooks/notebook1/#a-first-glimpse-of-python","title":"A First Glimpse of Python\u00b6","text":"<p>So far, we have used Jupyter to write text and mathematics.</p> <p>Now let\u2019s see one tiny example of Python.</p> <p>In the next cell, type it and run (Shift + Enter). Jupyter will show the result below.</p> <p>That\u2019s all for today \u2014 we will learn Python properly in the next few practical classes.</p>"}]}