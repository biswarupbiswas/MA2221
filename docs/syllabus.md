# Mathematics for Machine Learning  


## PART I — Mathematical Foundations

### Module 1 — Introduction
**Class 1**  
- What is Machine Learning  
- Data, models, training, prediction  
- Why mathematics is needed  

---

### Module 2 — Linear Algebra

**Class 2** – Scalars, vectors, matrices, notation  
**Class 3** – Linear systems and matrix operations  
**Class 4** – Matrix inverse and rank  
**Class 5** – Vector spaces and subspaces  
**Class 6** – Linear independence, basis, dimension  
**Class 7** – Column space and null space  
**Class 8** – Linear maps and geometric interpretation  

---

### Module 3 — Analytic Geometry

**Class 9** – Norms and distances  
**Class 10** – Inner products and angles  
**Class 11** – Orthogonality and orthonormal bases  
**Class 12** – Projections and least squares  
**Class 13** – Geometry of data  

---

### Module 4 — Matrix Decompositions (Chapter 4)

**Class 14** – Determinant and trace  
**Class 15** – Eigenvalues and eigenvectors  
**Class 16** – Diagonalization and spectral theorem  
**Class 17** – Singular Value Decomposition (SVD)  
**Class 18** – SVD for data representation  

---

### Module 5 — Vector Calculus

**Class 19** – Multivariate functions  
**Class 20** – Gradients and partial derivatives  
**Class 21** – Jacobian, Hessian, Taylor expansion  
**Class 22** – Gradient-based optimization  

---

### Module 6 — Probability

**Class 23** – Random variables and distributions  
**Class 24** – Expectation, variance, covariance  
**Class 25** – Multivariate Gaussian distribution  
**Class 26** – Probability and data modeling  

---

## PART II — Core Machine Learning

### Module 7 — When Models Meet Data

**Class 27** – Training vs testing and generalization  
**Class 28** – Loss functions and empirical risk  
**Class 29** – Overfitting, bias–variance, regularization  

---

### Module 8 — Linear Regression

**Class 30** – Linear regression model  
**Class 31** – Least-squares solution  
**Class 32** – Regularized regression  

---

### Module 9 — Principal Component Analysis

**Class 33** – Dimensionality reduction and covariance  
**Class 34** – Eigenvectors and principal components  
**Class 35** – PCA using SVD  
**Class 36** – PCA for compression and visualization  

---

### Module 10 — Gaussian Mixture Models

**Class 37** – Density estimation and mixture models  
**Class 38** – Gaussian Mixture Models  
**Class 39** – Expectation–Maximization (EM) algorithm  

---

### Module 11 — Support Vector Machines

**Class 40** – Maximum-margin classification and SVMs  

---

## Textbook

Deisenroth, M. P., Faisal, A. A., & Ong, C. S. (2020). **Mathematics for Machine Learning**. Cambridge: Cambridge University Press. 
